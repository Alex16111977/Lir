#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–ê–Ω–∞–ª–∏–∑ B1 —Å–ª–æ–≤–∞—Ä—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
–í–µ—Ä—Å–∏—è: 1.0 - 06.09.2025
"""

import json
from pathlib import Path
from collections import defaultdict

def analyze_b1_vocabulary():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π B1 —Å–ª–æ–≤–∞—Ä—å"""
    
    b1_path = Path(r'F:\AiKlientBank\Lir\data\b1')
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    all_words = []
    character_words = defaultdict(list)
    location_words = defaultdict(list) 
    emotion_words = defaultdict(list)
    gesture_words = defaultdict(list)
    
    # –î–∞–Ω–Ω—ã–µ –ø–æ —Ñ–∞–π–ª–∞–º
    file_data = {}
    
    for json_file in sorted(b1_path.glob('*.json')):
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            file_name = json_file.stem
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
            file_data[file_name] = {
                'title': data.get('title', ''),
                'icon': data.get('icon', ''),
                'quote': data.get('quote', ''),
                'words_count': 0
            }
            
            if 'vocabulary' in data:
                file_data[file_name]['words_count'] = len(data['vocabulary'])
                
                for word in data['vocabulary']:
                    word_text = word['german']
                    all_words.append(word_text)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
                    if 'character_voice' in word:
                        char = word['character_voice']['character']
                        character_words[char].append({
                            'word': word_text,
                            'context': word['character_voice']['german'],
                            'translation': word['translation'],
                            'file': file_name
                        })
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–∏ –∏ –∂–µ—Å—Ç—ã
                    if 'gesture' in word:
                        if 'emotion' in word['gesture']:
                            emotion_words[word['gesture']['emotion']].append(word_text)
                        if 'gesture' in word['gesture']:
                            gesture_words[word['gesture']['gesture']].append(word_text)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–æ–∫–∞—Ü–∏—é –ø–æ —Ñ–∞–π–ª—É
                    if '–¢—Ä–æ–Ω–Ω—ã–π' in file_name:
                        location = 'üè∞ –ó–∞–º–æ–∫'
                    elif '–ë—É—Ä—è' in file_name or '–±–µ–∑—É–º–∏–µ' in file_name:
                        location = '‚õàÔ∏è –ë—É—Ä—è'
                    elif '–î—É–≤—Ä' in file_name:
                        location = 'üèîÔ∏è –î—É–≤—Ä'
                    elif '–ë–∏—Ç–≤–∞' in file_name or '–î—É—ç–ª—å' in file_name:
                        location = '‚öîÔ∏è –ë–∏—Ç–≤–∞'
                    elif '–°–º–µ—Ä—Ç—å' in file_name:
                        location = '‚õìÔ∏è –¢—é—Ä—å–º–∞'
                    elif '–ü—Ä–∏–º–∏—Ä–µ–Ω–∏–µ' in file_name:
                        location = 'üíù –ü—Ä–∏–º–∏—Ä–µ–Ω–∏–µ'
                    elif '–ò–Ω—Ç—Ä–∏–≥–∞' in file_name or '–û–±–º–∞–Ω' in file_name:
                        location = 'üé≠ –ò–Ω—Ç—Ä–∏–≥–∞'
                    else:
                        location = 'üõ§Ô∏è –ü—É—Ç—å'
                    
                    location_words[location].append({
                        'word': word_text,
                        'translation': word['translation'],
                        'file': file_name
                    })
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = []
    report.append("="*70)
    report.append("–ê–ù–ê–õ–ò–ó B1 –°–õ–û–í–ê–†–Ø –î–õ–Ø –ö–ê–†–¢ –ü–ï–†–°–û–ù–ê–ñ–ï–ô")
    report.append("="*70)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    unique_words = list(set(all_words))
    report.append(f"\n[–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê]")
    report.append(f"–§–∞–π–ª–æ–≤: {len(file_data)}")
    report.append(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(all_words)}")
    report.append(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(unique_words)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º
    report.append(f"\n[–ü–ï–†–°–û–ù–ê–ñ–ò –ò –ò–• –°–õ–û–í–ê–†–¨]")
    report.append("-"*50)
    
    # –ì–ª–∞–≤–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    main_characters = {
        '–õ–∏—Ä': {'role': '–ö–æ—Ä–æ–ª—å', 'arc': '–≤–ª–∞—Å—Ç—å‚Üí–±–µ–∑—É–º–∏–µ‚Üí–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ', 'target': 400},
        '–ö–æ—Ä–¥–µ–ª–∏—è': {'role': '–ú–ª–∞–¥—à–∞—è –¥–æ—á—å', 'arc': '–ø—Ä–∞–≤–¥–∞‚Üí–∏–∑–≥–Ω–∞–Ω–∏–µ‚Üí–ø—Ä–æ—â–µ–Ω–∏–µ', 'target': 200},
        '–≠–¥–º—É–Ω–¥': {'role': '–ë–∞—Å—Ç–∞—Ä–¥', 'arc': '–∏–Ω—Ç—Ä–∏–≥–∞‚Üí–≤–ª–∞—Å—Ç—å‚Üí–ø–∞–¥–µ–Ω–∏–µ', 'target': 250},
        '–ì–ª–æ—Å—Ç–µ—Ä': {'role': '–ì—Ä–∞—Ñ', 'arc': '–¥–æ–≤–µ—Ä–∏–µ‚Üí–ø—Ä–æ–∑—Ä–µ–Ω–∏–µ‚Üí—Å—Ç—Ä–∞–¥–∞–Ω–∏–µ', 'target': 150},
        '–≠–¥–≥–∞—Ä': {'role': '–ó–∞–∫–æ–Ω–Ω—ã–π —Å—ã–Ω', 'arc': '–∏–∑–≥–Ω–∞–Ω–∏–µ‚Üí–±–µ–∑—É–º–∏–µ‚Üí—Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å', 'target': 150},
        '–ì–æ–Ω–µ—Ä–∏–ª—å—è': {'role': '–°—Ç–∞—Ä—à–∞—è –¥–æ—á—å', 'arc': '–ª–µ—Å—Ç—å‚Üí–∂–µ—Å—Ç–æ–∫–æ—Å—Ç—å‚Üí–≥–∏–±–µ–ª—å', 'target': 100},
        '–†–µ–≥–∞–Ω–∞': {'role': '–°—Ä–µ–¥–Ω—è—è –¥–æ—á—å', 'arc': '–ª–µ—Å—Ç—å‚Üí–∂–µ—Å—Ç–æ–∫–æ—Å—Ç—å‚Üí–≥–∏–±–µ–ª—å', 'target': 100},
        '–ö–µ–Ω—Ç': {'role': '–í–µ—Ä–Ω—ã–π –≥—Ä–∞—Ñ', 'arc': '–≤–µ—Ä–Ω–æ—Å—Ç—å‚Üí–∏–∑–≥–Ω–∞–Ω–∏–µ‚Üí–≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ', 'target': 100},
        '–®—É—Ç': {'role': '–®—É—Ç –∫–æ—Ä–æ–ª—è', 'arc': '–º—É–¥—Ä–æ—Å—Ç—å‚Üí–≤–µ—Ä–Ω–æ—Å—Ç—å‚Üí–∏—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ', 'target': 150}
    }
    
    for char_name, char_info in main_characters.items():
        if char_name in character_words:
            words = character_words[char_name]
            unique_char_words = list(set([w['word'] for w in words]))
            report.append(f"\n{char_name} ({char_info['role']})")
            report.append(f"  –ê—Ä–∫–∞: {char_info['arc']}")
            report.append(f"  –°–ª–æ–≤ —Å–µ–π—á–∞—Å: {len(unique_char_words)} / –¶–µ–ª—å: {char_info['target']}")
            report.append(f"  –ü—Ä–æ—Ü–µ–Ω—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏: {len(unique_char_words)*100//char_info['target']}%")
            
            # –ü—Ä–∏–º–µ—Ä—ã —Å–ª–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
            if len(words) > 0:
                report.append(f"  –ü—Ä–∏–º–µ—Ä—ã:")
                for w in words[:3]:
                    report.append(f"    - {w['word']} ({w['translation']})")
                    report.append(f"      \"{w['context']}\"")
        else:
            report.append(f"\n{char_name} ({char_info['role']})")
            report.append(f"  [!] –ù–ï–¢ –í –°–õ–û–í–ê–†–ï - –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å {char_info['target']} —Å–ª–æ–≤")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ª–æ–∫–∞—Ü–∏—è–º
    report.append(f"\n\n[–ö–ê–†–¢–ê –ü–£–¢–ï–®–ï–°–¢–í–ò–Ø –õ–ò–†–ê]")
    report.append("-"*50)
    
    journey_order = ['üè∞ –ó–∞–º–æ–∫', 'üé≠ –ò–Ω—Ç—Ä–∏–≥–∞', '‚õàÔ∏è –ë—É—Ä—è', 'üèîÔ∏è –î—É–≤—Ä', 'üíù –ü—Ä–∏–º–∏—Ä–µ–Ω–∏–µ', '‚öîÔ∏è –ë–∏—Ç–≤–∞', '‚õìÔ∏è –¢—é—Ä—å–º–∞']
    
    for location in journey_order:
        if location in location_words:
            words = location_words[location]
            unique_loc_words = list(set([w['word'] for w in words]))
            report.append(f"\n{location}: {len(unique_loc_words)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª–∞–º
            files_in_location = set([w['file'] for w in words])
            report.append(f"  –°—Ü–µ–Ω—ã ({len(files_in_location)}):")
            for file_name in sorted(files_in_location):
                if file_name in file_data:
                    report.append(f"    - {file_data[file_name]['icon']} {file_name}")
    
    # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–∞—Ä—Ç–∞
    report.append(f"\n\n[–≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –ö–ê–†–¢–ê]")
    report.append("-"*50)
    
    top_emotions = sorted(emotion_words.items(), key=lambda x: len(x[1]), reverse=True)[:15]
    for emotion, words in top_emotions:
        report.append(f"{emotion:30} {len(words):3} —Å–ª–æ–≤")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    report.append(f"\n\n[–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –†–ï–ê–õ–ò–ó–ê–¶–ò–ò]")
    report.append("="*70)
    
    report.append("\n1. –î–û–ü–û–õ–ù–ò–¢–¨ –ü–ï–†–°–û–ù–ê–ñ–ï–ô:")
    missing_words_total = 0
    for char_name, char_info in main_characters.items():
        current = len(set([w['word'] for w in character_words.get(char_name, [])]))
        needed = char_info['target'] - current
        if needed > 0:
            report.append(f"   - {char_name}: –¥–æ–±–∞–≤–∏—Ç—å {needed} —Å–ª–æ–≤")
            missing_words_total += needed
    
    report.append(f"\n   –ò–¢–û–ì–û –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: {missing_words_total} —Å–ª–æ–≤")
    
    report.append("\n2. –°–¢–†–£–ö–¢–£–†–ê –ö–ê–†–¢ –ü–ï–†–°–û–ù–ê–ñ–ï–ô:")
    report.append("   - –í–∑—è—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ {0} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤".format(len(unique_words)))
    report.append("   - –†–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ –∏—Ö –∞—Ä–∫–∞–º")
    report.append("   - –î–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Å–ª–æ–≤–∞ –∏–∑ dictionary_b1.py")
    report.append("   - –°–æ–∑–¥–∞—Ç—å character_maps.json —Å –ø—Ä–∏–≤—è–∑–∫–∞–º–∏")
    
    report.append("\n3. –¢–ï–•–ù–ò–ß–ï–°–ö–ò:")
    report.append("   - –°–æ–∑–¥–∞—Ç—å CharacterVocabularyGenerator")
    report.append("   - –û–±–æ–≥–∞—Ç–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ JSON character_voice")
    report.append("   - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º")
    report.append("   - –î–æ–±–∞–≤–∏—Ç—å –Ω–∞–≤–∏–≥–∞—Ü–∏—é –ø–æ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—é –õ–∏—Ä–∞")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    output_path = Path(r'F:\AiKlientBank\Lir\test\b1_character_analysis.txt')
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))
    
    print(f"[OK] –ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    return {
        'unique_words': len(unique_words),
        'characters': character_words,
        'locations': location_words,
        'emotions': emotion_words,
        'files': file_data
    }

if __name__ == "__main__":
    result = analyze_b1_vocabulary()
    print(f"[INFO] –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {result['unique_words']}")
    print(f"[INFO] –ü–µ—Ä—Å–æ–Ω–∞–∂–µ–π: {len(result['characters'])}")
    print(f"[INFO] –õ–æ–∫–∞—Ü–∏–π: {len(result['locations'])}")
